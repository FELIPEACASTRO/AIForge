# Soft Prompting

## Description

**Soft Prompting** (tamb√©m conhecidos como *learned prompts*, *continuous prompts* ou *prompt embeddings*) √© uma t√©cnica de **Parameter-Efficient Fine-Tuning (PEFT)** que adapta Large Language Models (LLMs) pr√©-treinados para tarefas espec√≠ficas sem a necessidade de treinar todos os seus par√¢metros. Ao contr√°rio dos **Hard Prompts** (prompts textuais discretos e criados manualmente), os Soft Prompts s√£o **tensores aprend√≠veis** (vetores de *tokens virtuais*) que s√£o concatenados com os *embeddings* de entrada do modelo e otimizados diretamente em um conjunto de dados de treinamento.

Essa abordagem permite que o modelo permane√ßa congelado, enquanto apenas um pequeno conjunto de par√¢metros do prompt √© treinado, resultando em uma adapta√ß√£o significativamente mais eficiente em termos de tempo e custo computacional. O principal ponto negativo √© que esses *tokens virtuais* n√£o s√£o leg√≠veis por humanos.

## Statistics

*   **Efici√™ncia de Par√¢metros:** Prefix Tuning demonstrou desempenho compar√°vel ao *fine-tuning* completo, mas com **1000x menos par√¢metros** trein√°veis.
*   **Escalabilidade:** O desempenho do Prompt Tuning se **escala** com o aumento do tamanho do modelo, equiparando-se ao *fine-tuning* tradicional em modelos maiores.
*   **Pesquisa Recente (2024):** O trabalho "Nemesis: Normalizing the Soft-prompt Vectors of Vision-Language Models" (ICLR 2024) investigou o **Efeito de Baixa Norma (*Low-Norm Effect*)** em *soft-prompts* para Vision-Language Models (VLMs), sugerindo que a redu√ß√£o da norma de certos prompts aprendidos pode **melhorar o desempenho** dos VLMs.

## Features

O Soft Prompting engloba v√°rias sub-t√©cnicas de PEFT, cada uma com varia√ß√µes na forma como os *embeddings* do prompt s√£o inseridos e otimizados:

1.  **Prompt Tuning:** Adiciona *tokens* de prompt aprend√≠veis apenas aos *embeddings* de entrada. Bom para classifica√ß√£o de texto e escal√°vel com o tamanho do modelo.
2.  **Prefix Tuning:** Insere par√¢metros de prefixo otimiz√°veis em **todas** as camadas do modelo. Ideal para Gera√ß√£o de Linguagem Natural (NLG).
3.  **P-Tuning:** Utiliza um codificador de prompt (como LSTM) e permite que os *tokens* de prompt sejam inseridos em **qualquer lugar** na sequ√™ncia de entrada. Projetado para Compreens√£o de Linguagem Natural (NLU).
4.  **Multitask Prompt Tuning (MPT):** Aprende um √∫nico prompt para m√∫ltiplos tipos de tarefas, permitindo *transfer learning* eficiente.
5.  **Context-Aware Prompt Tuning (CPT):** Refina apenas *embeddings* de *tokens* de contexto espec√≠ficos para aprimorar a classifica√ß√£o *few-shot*.

## Use Cases

*   **Adapta√ß√£o de Modelos:** Adapta√ß√£o eficiente de LLMs pr√©-treinados para uma ampla variedade de tarefas *downstream* (ex: classifica√ß√£o, gera√ß√£o, NLU) sem a necessidade de *fine-tuning* completo.
*   **Ambientes de Baixa Quantidade de Dados:** Prefix Tuning √© particularmente eficaz em cen√°rios com poucos dados (*low-data settings*).
*   **Transfer Learning:** Multitask Prompt Tuning permite o *transfer learning* de um √∫nico prompt aprendido para m√∫ltiplas tarefas.
*   **Modelos Multimodais:** Pesquisas recentes aplicam Soft Prompting em Vision-Language Models (VLMs) como o CLIP para adapta√ß√£o de tarefas.

## Integration

Como os Soft Prompts s√£o tensores aprend√≠veis e n√£o texto leg√≠vel por humanos, n√£o h√° "exemplos de prompt" no sentido tradicional de texto de entrada. A integra√ß√£o se d√° atrav√©s da implementa√ß√£o de uma das sub-t√©cnicas (PEFT).

**Melhores Pr√°ticas (PEFT):**
*   **Escolha da T√©cnica:** A escolha da sub-t√©cnica depende da tarefa: **Prompt Tuning** para classifica√ß√£o, **Prefix Tuning** para NLG e **P-Tuning** para NLU.
*   **Implementa√ß√£o:** Utilizar bibliotecas como **ü§ó PEFT (Parameter-Efficient Fine-Tuning)** da Hugging Face, que fornece implementa√ß√µes prontas.
*   **Otimiza√ß√£o:** A otimiza√ß√£o √© feita via *backpropagation* no conjunto de dados de treinamento, atualizando apenas os par√¢metros do prompt enquanto o modelo base permanece congelado.

## URL

https://huggingface.co/docs/peft/en/conceptual_guides/prompting