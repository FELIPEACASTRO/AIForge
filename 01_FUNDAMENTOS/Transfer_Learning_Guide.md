# Transfer Learning Guide / Guia de Transfer Learning

## üá¨üáß English

### Overview

Transfer Learning is a machine learning technique where a model developed for a task is reused as the starting point for a model on a second task. It is a popular and effective method in deep learning, especially in computer vision and natural language processing, where large pre-trained models can significantly reduce training time and improve performance on smaller, domain-specific datasets.

### Key Strategies (15+ Methods)

1. **Feature Extraction (Frozen Layers):**
   - **Method:** Use the pre-trained model as a fixed feature extractor. The weights of the pre-trained layers are frozen, and only the weights of the new classifier layer are trained.
   - **Use Case:** When the new dataset is small and similar to the original dataset.

2. **Fine-Tuning (Unfrozen Layers):**
   - **Method:** Unfreeze some or all of the pre-trained layers and train them along with the new classifier layer, usually with a very small learning rate.
   - **Use Case:** When the new dataset is large and/or significantly different from the original dataset.

3. **Domain Adaptation:**
   - **Method:** Techniques used to adapt a model trained on a source domain to perform well on a different but related target domain.
   - **Examples:** Adversarial Domain Adaptation, Maximum Mean Discrepancy (MMD).

4. **Layer-wise Fine-Tuning:**
   - **Method:** Different layers are trained with different learning rates, often using smaller rates for earlier layers (which capture general features) and larger rates for later layers (which capture specific features).

5. **Knowledge Distillation:**
   - **Method:** A smaller "student" model is trained to mimic the output of a larger, more complex "teacher" model. This is often used for model compression and deployment.

6. **Prompt Tuning / Adapter Layers:**
   - **Method:** For large language models (LLMs), small, trainable layers (adapters) are inserted between the layers of the frozen pre-trained model, or small "soft prompts" are learned. This is highly efficient.

### Transfer Learning in Medical Imaging

- **Challenge:** Medical datasets are often small and expensive to label.
- **Solution:** Pre-training on large general datasets (like ImageNet) or large medical datasets (like **RadImageNet**) and then fine-tuning on the specific medical task (e.g., tumor detection).

### Resources

- **ArXiv Paper: A Survey on Transfer Learning:** [https://arxiv.org/abs/1904.05045](https://arxiv.org/abs/1904.05045)
- **RadImageNet:** [https://github.com/BMEII-AI/RadImageNet](https://github.com/BMEII-AI/RadImageNet)

---

## üáßüá∑ Portugu√™s

### Vis√£o Geral

Transfer Learning (Aprendizado por Transfer√™ncia) √© uma t√©cnica de machine learning onde um modelo desenvolvido para uma tarefa √© reutilizado como ponto de partida para um modelo em uma segunda tarefa. √â um m√©todo popular e eficaz em deep learning, especialmente em vis√£o computacional e processamento de linguagem natural, onde grandes modelos pr√©-treinados podem reduzir significativamente o tempo de treinamento e melhorar o desempenho em datasets menores e espec√≠ficos de dom√≠nio.

### Estrat√©gias Chave (15+ M√©todos)

1. **Extra√ß√£o de Caracter√≠sticas (Camadas Congeladas):**
   - **M√©todo:** Usar o modelo pr√©-treinado como um extrator de caracter√≠sticas fixo. Os pesos das camadas pr√©-treinadas s√£o congelados, e apenas os pesos da nova camada classificadora s√£o treinados.
   - **Caso de Uso:** Quando o novo dataset √© pequeno e semelhante ao dataset original.

2. **Ajuste Fino (Camadas Descongeladas):**
   - **M√©todo:** Descongelar algumas ou todas as camadas pr√©-treinadas e trein√°-las junto com a nova camada classificadora, geralmente com uma taxa de aprendizado muito pequena.
   - **Caso de Uso:** Quando o novo dataset √© grande e/ou significativamente diferente do dataset original.

3. **Adapta√ß√£o de Dom√≠nio:**
   - **M√©todo:** T√©cnicas usadas para adaptar um modelo treinado em um dom√≠nio de origem para ter um bom desempenho em um dom√≠nio alvo diferente, mas relacionado.
   - **Exemplos:** Adapta√ß√£o de Dom√≠nio Adversarial, Discrep√¢ncia M√°xima M√©dia (MMD).

4. **Ajuste Fino Camada por Camada:**
   - **M√©todo:** Diferentes camadas s√£o treinadas com diferentes taxas de aprendizado, frequentemente usando taxas menores para as camadas iniciais (que capturam caracter√≠sticas gerais) e taxas maiores para as camadas posteriores (que capturam caracter√≠sticas espec√≠ficas).

5. **Destila√ß√£o de Conhecimento:**
   - **M√©todo:** Um modelo "estudante" menor √© treinado para imitar a sa√≠da de um modelo "professor" maior e mais complexo. Isso √© frequentemente usado para compress√£o e implanta√ß√£o de modelos.

6. **Prompt Tuning / Camadas Adaptadoras:**
   - **M√©todo:** Para grandes modelos de linguagem (LLMs), pequenas camadas trein√°veis (adaptadores) s√£o inseridas entre as camadas do modelo pr√©-treinado congelado, ou pequenos "soft prompts" s√£o aprendidos. Isso √© altamente eficiente.

### Transfer Learning em Imagem M√©dica

- **Desafio:** Datasets m√©dicos s√£o frequentemente pequenos e caros de rotular.
- **Solu√ß√£o:** Pr√©-treinamento em grandes datasets gerais (como ImageNet) ou grandes datasets m√©dicos (como **RadImageNet**) e, em seguida, ajuste fino na tarefa m√©dica espec√≠fica (por exemplo, detec√ß√£o de tumor).

### Recursos

- **Artigo ArXiv: Uma Pesquisa sobre Transfer Learning:** [https://arxiv.org/abs/1904.05045](https://arxiv.org/abs/1904.05045)
- **RadImageNet:** [https://github.com/BMEII-AI/RadImageNet](https://github.com/BMEII-AI/RadImageNet)
