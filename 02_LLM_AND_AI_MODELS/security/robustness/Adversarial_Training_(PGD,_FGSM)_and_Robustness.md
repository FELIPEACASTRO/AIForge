# Adversarial Training (PGD, FGSM) and Robustness

## Description

Adversarial Training (AT) is a defense mechanism and training paradigm for Deep Neural Networks (DNNs) designed to enhance **adversarial robustness**. It is formulated as a min-max optimization problem: $\min_{\theta} \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \max_{\delta \in \mathcal{S}} L(\theta, x + \delta, y) \right]$. The inner maximization finds the worst-case adversarial perturbation ($\delta$) within a constrained set ($\mathcal{S}$), while the outer minimization optimizes the model parameters ($\theta$) to minimize the loss on these perturbed examples. The two primary methods for generating these perturbations are the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD). FGSM is a single-step, computationally efficient attack, often used in **Fast Adversarial Training**, but can suffer from catastrophic overfitting. PGD is a multi-step, iterative attack considered the **gold standard** for evaluating and achieving strong adversarial robustness, as it leads to models with genuinely smoother decision boundaries. Adversarial Robustness is the model's ability to maintain performance against these imperceptible, malicious inputs.

## Statistics

**Robust Accuracy:** The primary metric, measuring the model's accuracy on adversarial examples generated by a strong attack (e.g., PGD). **Clean Accuracy:** The model's accuracy on unperturbed data; often exhibits a trade-off with robust accuracy. **Perturbation Budget ($\epsilon$):** The maximum allowed magnitude of the adversarial perturbation (e.g., $\epsilon=8/255$ for $L_\infty$ norm on image data). **Attack Success Rate (ASR):** The percentage of adversarial examples that successfully fool the model, used to measure the strength of an attack or the vulnerability of a non-robust model.

## Features

**PGD-based Robustness:** Achieves the strongest, most reliable adversarial robustness by using a multi-step, iterative attack during training, resulting in smoother loss landscapes. **Fast Adversarial Training (FGSM):** Significantly reduces training time compared to multi-step methods, though it requires techniques to mitigate catastrophic overfitting. **Enhanced Security:** Protects models against both white-box and black-box adversarial attacks in security-critical applications. **Regularization Effect:** AT can act as a strong regularizer, potentially improving model generalization on clean data in certain scenarios.

## Use Cases

**Autonomous Systems:** Ensuring the reliability of perception models in self-driving cars against subtle, malicious alterations to road signs or objects. **Medical Imaging:** Preventing misdiagnosis in critical healthcare systems where an attacker could subtly alter an X-ray or MRI scan to force a misclassification. **Security and Fraud Detection:** Creating robust classifiers for spam, malware, or financial fraud that are difficult for adversaries to evade by slightly modifying malicious inputs. **E-commerce and Recommendation Systems:** Generating negative training examples to improve the ranking and relevance of search results and product discovery, as demonstrated by Amazon Science.

## Integration

Adversarial Training is implemented using deep learning frameworks like PyTorch and TensorFlow, often utilizing specialized libraries.

**PyTorch (Conceptual PGD-AT Loop):**
```python
def pgd_attack(model, X, y, epsilon, alpha, num_steps):
    X_adv = X.clone().detach()
    # ... PGD iterative steps (gradient calculation, step, projection) ...
    return X_adv

# Adversarial Training Loop
for X, y in dataloader:
    X_adv = pgd_attack(model, X, y, epsilon=0.03, alpha=0.007, num_steps=10)
    optimizer.zero_grad()
    loss_adv = criterion(model(X_adv), y)
    loss_adv.backward()
    optimizer.step()
```

**TensorFlow (Neural Structured Learning - NSL):**
NSL provides a high-level API for simplified implementation.
```python
import neural_structured_learning as nsl

adv_config = nsl.configs.make_adv_reg_config(
    multiplier=0.2, 
    adv_step_size=0.005, 
    adv_grad_norm='linf'
)
adv_model = nsl.keras.AdversarialRegularization(base_model, adv_config=adv_config)

adv_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
adv_model.fit(x=train_dataset, y=train_labels, epochs=10)
```
**Toolboxes:** Adversarial Robustness Toolbox (ART) and CleverHans are popular libraries that abstract these implementations.

## URL

https://www.tensorflow.org/neural_structured_learning (TensorFlow NSL); https://github.com/Trusted-AI/adversarial-robustness-toolbox (ART)