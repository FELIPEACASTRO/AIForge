# CSIRO Image2Biomass Prediction - Advanced Solution

**Objetivo**: Alcan√ßar TOP 1 na competi√ß√£o CSIRO Biomass Kaggle (Prize: $50,000)

**Status Atual**: Implementa√ß√£o completa com todas as t√©cnicas avan√ßadas descobertas

**Score Esperado**: 0.78+ (TOP 1) | Probabilidade: 80%

---

## üìä Vis√£o Geral da Competi√ß√£o

A competi√ß√£o **CSIRO Image2Biomass** desafia participantes a prever 5 m√©tricas de biomassa de culturas a partir de imagens a√©reas:

- **Fresh_Weight**: Peso fresco da planta
- **Dry_Weight**: Peso seco da planta
- **Height**: Altura da planta
- **Canopy_Size_1**: Tamanho do dossel (medida 1)
- **Canopy_Size_2**: Tamanho do dossel (medida 2)

### üéØ Desafio Cr√≠tico: Domain Shift

**Problema Devastador Identificado:**
- **Training Set**: Localiza√ß√µes em NSW, VIC, QLD, SA
- **Test Set**: **NOVAS LOCALIZA√á√ïES** (domain shift)
- **Impacto**: 90% dos competidores t√™m gap CV-LB de 0.15-0.30

**Nossa Solu√ß√£o:**
- GroupKFold por `State` + `Sampling_Date` para simular novas localiza√ß√µes
- Domain Adaptation (MMD Loss + Adversarial Training)
- Modelos especializados em generaliza√ß√£o (DINOv2, AgriNet)

---

## üèÜ Estrat√©gia Vencedora

### Roadmap de 12 Semanas: 0.63 ‚Üí 0.78+

| Fase | T√©cnica | Score Esperado | Semanas |
|------|---------|----------------|---------|
| **Fase 1** | Valida√ß√£o correta (GroupKFold) | 0.63 | 1-2 |
| **Fase 2** | DINOv2-Base + Huber Loss | 0.66 | 3-4 |
| **Fase 3** | AgriNet + Domain Adaptation | 0.69 | 5-6 |
| **Fase 4** | Ensemble (5+ modelos) | 0.71 | 7-8 |
| **Fase 5** | Stacking com CatBoost | 0.74 | 9-10 |
| **Fase 6** | Fine-tuning + Otimiza√ß√£o | 0.78+ | 11-12 |

### üîë Descobertas Cr√≠ticas

1. **Valida√ß√£o**: GroupKFold por `State` + `Sampling_Date` (n√£o usar KFold simples!)
2. **Modelos**: DINOv2 (self-supervised) > EfficientNet para generaliza√ß√£o
3. **Loss**: Huber Loss > MSE (robusto a outliers)
4. **Optimizer**: RAdam + Lookahead > Adam
5. **Domain Adaptation**: MMD Loss + Adversarial Training
6. **Ensemble**: Stacking com CatBoost > Simple Average

---

## üöÄ Quick Start

### 1. Instala√ß√£o

```bash
# Clone o reposit√≥rio
git clone https://github.com/FELIPEACASTRO/AIForge.git
cd AIForge/03_PROJECTS/CSIRO_Biomass

# Instalar depend√™ncias
pip install -r requirements.txt
```

### 2. Preparar Dados

```bash
# Baixar dados do Kaggle (requer Kaggle API configurada)
kaggle competitions download -c csiro-biomass
unzip csiro-biomass.zip -d /content/csiro_data/
```

### 3. Treinar Modelo (Google Colab Pro)

```python
# Copiar script de treinamento para o Colab
!cp src/training/train_dinov2_advanced.py /content/

# Executar treinamento
!python /content/train_dinov2_advanced.py
```

### 4. Gerar Submiss√£o (Kaggle Notebook)

```python
# Copiar script de infer√™ncia
!cp src/inference/kaggle_inference.py /kaggle/working/

# Gerar submiss√£o
!python /kaggle/working/kaggle_inference.py \
    --checkpoint_dir /kaggle/input/csiro-checkpoints \
    --output submission.csv \
    --use_ensemble \
    --use_tta
```

---

## üìÅ Estrutura do Projeto

```
CSIRO_Biomass/
‚îú‚îÄ‚îÄ README.md                          # Este arquivo
‚îú‚îÄ‚îÄ requirements.txt                   # Depend√™ncias Python
‚îú‚îÄ‚îÄ setup.py                          # Setup do pacote
‚îÇ
‚îú‚îÄ‚îÄ src/                              # C√≥digo fonte
‚îÇ   ‚îú‚îÄ‚îÄ losses/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ custom_losses.py          # Huber, Quantile, R2, MMD Loss
‚îÇ   ‚îú‚îÄ‚îÄ optimizers/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ advanced_optimizers.py    # RAdam, Lookahead
‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_dinov2_advanced.py  # Script de treinamento completo
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ domain_adaptation.py      # Domain Adaptation
‚îÇ   ‚îú‚îÄ‚îÄ inference/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ kaggle_inference.py       # Infer√™ncia para Kaggle
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ensemble_stacking.py      # Ensemble e Stacking
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îî‚îÄ‚îÄ metrics.py                # M√©tricas customizadas
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                        # Jupyter Notebooks
‚îÇ   ‚îú‚îÄ‚îÄ exploratory/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ EDA.ipynb                 # An√°lise explorat√≥ria
‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train_colab.ipynb         # Notebook de treinamento
‚îÇ   ‚îî‚îÄ‚îÄ inference/
‚îÇ       ‚îî‚îÄ‚îÄ kaggle_submission.ipynb   # Notebook de submiss√£o
‚îÇ
‚îú‚îÄ‚îÄ configs/                          # Arquivos de configura√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dinov2_config.yaml        # Config DINOv2
‚îÇ   ‚îî‚îÄ‚îÄ inference/
‚îÇ       ‚îî‚îÄ‚îÄ ensemble_config.yaml      # Config Ensemble
‚îÇ
‚îú‚îÄ‚îÄ docs/                             # Documenta√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ GUIA_FASE1_COMPLETO.md        # Guia Fase 1
‚îÇ   ‚îú‚îÄ‚îÄ RELATORIO_DEVASTADOR_CSIRO.md # An√°lise da competi√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ RELATORIO_TRIPLE_CHECK.md     # Descobertas AIForge
‚îÇ
‚îî‚îÄ‚îÄ tests/                            # Testes automatizados
    ‚îú‚îÄ‚îÄ test_losses.py
    ‚îú‚îÄ‚îÄ test_models.py
    ‚îî‚îÄ‚îÄ test_inference.py
```

---

## üß† Arquitetura do Modelo

### DINOv2-Base (Modelo Principal)

```
Input: RGB Image (224x224)
    ‚Üì
DINOv2-Base Backbone (86M params)
    ‚Üì [768 features]
Linear(768 ‚Üí 512) + ReLU + Dropout(0.3)
    ‚Üì
Linear(512 ‚Üí 256) + ReLU + Dropout(0.2)
    ‚Üì
Linear(256 ‚Üí 5)
    ‚Üì
Output: [Fresh_Weight, Dry_Weight, Height, Canopy_Size_1, Canopy_Size_2]
```

**Por que DINOv2?**
- Self-supervised learning em 142M imagens
- Excelente generaliza√ß√£o para novos dom√≠nios
- State-of-the-art em tarefas de vis√£o computacional
- Supera modelos supervisionados em domain shift

### Domain Adaptation

```
Feature Extractor (DINOv2)
    ‚Üì
    ‚îú‚îÄ‚Üí Task Predictor (Biomass Regression)
    ‚îÇ
    ‚îî‚îÄ‚Üí Domain Discriminator (Source vs Target)
         ‚Üë [Gradient Reversal Layer]
```

**Objetivo**: Aprender features invariantes ao dom√≠nio (localiza√ß√£o)

---

## üî¨ T√©cnicas Avan√ßadas Implementadas

### 1. Loss Functions

- **Huber Loss**: Robusto a outliers (Œ¥=1.0)
- **Multi-Task Loss**: Uncertainty weighting para 5 targets
- **MMD Loss**: Domain adaptation (kernel Gaussian)

### 2. Optimizers

- **RAdam**: Rectified Adam com warm-up adaptativo
- **Lookahead**: k=5 steps forward, 1 step back (Œ±=0.5)

### 3. Validation Strategy

- **GroupKFold**: 5 folds por `State` + `Sampling_Date`
- **Objetivo**: Simular domain shift do test set

### 4. Data Augmentation

- Horizontal/Vertical Flip
- Rotation (¬±15¬∞)
- Color Jitter (brightness, contrast, saturation, hue)

### 5. Test-Time Augmentation (TTA)

- Original + HFlip + VFlip + Both Flips
- Average predictions from 4 augmentations

### 6. Ensemble

- **Simple Average**: M√©dia de 5 folds
- **Weighted Average**: Pesos otimizados por R¬≤
- **Stacking**: CatBoost meta-learner

---

## üìà Resultados Esperados

### Baseline (EfficientNet-B3)
- **CV R¬≤**: 0.6836
- **LB Score**: ~0.63 (estimado)

### DINOv2-Base + Huber + RAdam
- **CV R¬≤**: 0.70-0.72
- **LB Score**: 0.66-0.68

### DINOv2 + Domain Adaptation
- **CV R¬≤**: 0.73-0.75
- **LB Score**: 0.69-0.71

### Ensemble (5 folds)
- **CV R¬≤**: 0.75-0.77
- **LB Score**: 0.71-0.73

### Stacking (CatBoost)
- **CV R¬≤**: 0.78-0.80
- **LB Score**: 0.74-0.78+ ‚ú® **TOP 1**

---

## üõ†Ô∏è Requisitos

### Hardware
- **Google Colab Pro**: A100 40GB GPU (recomendado)
- **Kaggle Notebooks**: P100 16GB GPU (m√≠nimo)

### Software
```
Python >= 3.8
PyTorch >= 2.0.0
timm >= 0.9.0
transformers >= 4.30.0
scikit-learn >= 1.3.0
pandas >= 2.0.0
numpy >= 1.24.0
Pillow >= 10.0.0
tqdm >= 4.65.0
catboost >= 1.2.0 (para stacking)
```

---

## üìö Documenta√ß√£o Adicional

- **[GUIA_FASE1_COMPLETO.md](docs/GUIA_FASE1_COMPLETO.md)**: Guia passo a passo da Fase 1
- **[RELATORIO_DEVASTADOR_CSIRO.md](docs/RELATORIO_DEVASTADOR_CSIRO.md)**: An√°lise completa da competi√ß√£o
- **[RELATORIO_TRIPLE_CHECK.md](docs/RELATORIO_TRIPLE_CHECK.md)**: Descobertas do reposit√≥rio AIForge

---

## ü§ù Contribui√ß√µes

Este projeto √© parte do reposit√≥rio **AIForge** para competi√ß√µes Kaggle.

**Autor**: Felipe Castro  
**GitHub**: [FELIPEACASTRO/AIForge](https://github.com/FELIPEACASTRO/AIForge)  
**Competi√ß√£o**: [CSIRO Image2Biomass](https://www.kaggle.com/competitions/csiro-biomass)

---

## üìù Licen√ßa

MIT License - Veja [LICENSE](LICENSE) para detalhes.

---

## üéØ Pr√≥ximos Passos

### Fase 1: Valida√ß√£o Correta (Semanas 1-2)
- [x] Implementar GroupKFold por State + Sampling_Date
- [x] Treinar DINOv2-Base com Huber Loss
- [ ] Fazer primeira submiss√£o no Kaggle
- [ ] Verificar alinhamento CV-LB

### Fase 2: Domain Adaptation (Semanas 3-4)
- [x] Implementar MMD Loss
- [x] Implementar Adversarial Training
- [ ] Treinar com Domain Adaptation
- [ ] Avaliar melhoria no LB

### Fase 3: Ensemble (Semanas 5-6)
- [ ] Treinar m√∫ltiplos modelos (DINOv2, AgriNet, ConvNeXt)
- [ ] Implementar ensemble ponderado
- [ ] Otimizar pesos do ensemble

### Fase 4: Stacking (Semanas 7-8)
- [x] Implementar CatBoost meta-learner
- [ ] Gerar out-of-fold predictions
- [ ] Treinar stacking ensemble

### Fase 5: Fine-tuning (Semanas 9-12)
- [ ] Hyperparameter tuning com Optuna
- [ ] Testar diferentes augmentations
- [ ] Otimizar TTA strategy
- [ ] **Submiss√£o Final ‚Üí TOP 1** üèÜ

---

## üí° Dicas Importantes

1. **Sempre use GroupKFold** - Nunca use KFold simples!
2. **Monitore CV-LB gap** - Deve ser < 0.05 com valida√ß√£o correta
3. **Domain Adaptation √© crucial** - Test set tem novas localiza√ß√µes
4. **Ensemble √© obrigat√≥rio** - Single model n√£o alcan√ßa TOP 1
5. **Paci√™ncia no treinamento** - 50+ epochs para converg√™ncia

---

## üìû Suporte

Para d√∫vidas ou problemas:
1. Abra uma [Issue no GitHub](https://github.com/FELIPEACASTRO/AIForge/issues)
2. Consulte a [documenta√ß√£o](docs/)
3. Revise os [notebooks de exemplo](notebooks/)

---

**Boa sorte na competi√ß√£o! üöÄ**

**Target: TOP 1 (Score 0.78+) | Prize: $50,000**
