# MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models

## üá¨üáß English

### Overview

MegaScale-MoE is a production system for efficient training of large-scale Mixture-of-Experts (MoE) models (352B parameters on 1,440 NVIDIA Hopper GPUs). MegaScale-MoE optimizes communication and overlaps communication with computation, achieving a 1.88x efficiency increase compared to Megatron-LM. It is a game-changer for training next-generation MoE LLMs.

### Key Features

- **Communication-Efficient Training:** Optimizes communication and overlaps it with computation.
- **Large-Scale Training:** Designed for training massive MoE models (352B parameters).
- **High Efficiency:** Achieves a 1.88x efficiency increase compared to Megatron-LM.

### Impact

MegaScale-MoE is a game-changer for the training of large-scale MoE models. By making the training process more efficient, it enables the development of even larger and more powerful MoE models, pushing the boundaries of what is possible with LLMs.

- **Source:** [ArXiv (May 2025)](https://arxiv.org/html/2505.11432v3)

---

## üáßüá∑ Portugu√™s

### Vis√£o Geral

O MegaScale-MoE √© um sistema de produ√ß√£o para treinamento eficiente de modelos de Mistura de Especialistas (MoE) em larga escala (352B par√¢metros em 1.440 GPUs NVIDIA Hopper). O MegaScale-MoE otimiza a comunica√ß√£o e sobrep√µe a comunica√ß√£o com a computa√ß√£o, alcan√ßando um aumento de efici√™ncia de 1.88x em compara√ß√£o com o Megatron-LM. √â um divisor de √°guas para o treinamento de LLMs MoE de pr√≥xima gera√ß√£o.

### Caracter√≠sticas Principais

- **Treinamento Eficiente em Comunica√ß√£o:** Otimiza a comunica√ß√£o e a sobrep√µe com a computa√ß√£o.
- **Treinamento em Larga Escala:** Projetado para o treinamento de modelos MoE massivos (352B par√¢metros).
- **Alta Efici√™ncia:** Atinge um aumento de efici√™ncia de 1.88x em compara√ß√£o com o Megatron-LM.

### Impacto

O MegaScale-MoE √© um divisor de √°guas para o treinamento de modelos MoE em larga escala. Ao tornar o processo de treinamento mais eficiente, ele permite o desenvolvimento de modelos MoE ainda maiores e mais poderosos, expandindo os limites do que √© poss√≠vel com LLMs.

- **Fonte:** [ArXiv (Maio de 2025)](https://arxiv.org/html/2505.11432v3)
