# üß† Deep Learning - Arquiteturas, T√©cnicas e Papers / Architectures, Techniques, and Papers

## üá¨üáß English

### Overview

This is the most comprehensive collection of Deep Learning resources, architectures, and techniques, compiled from a massive, record-breaking search. It covers the evolution of key concepts from 2012 to 2025, including Transformers, CNNs, Self-Supervised Learning, and AutoML.

### Key Sections

| Section | Description | Key Highlights |
| :--- | :--- | :--- |
| **AutoML & NAS** | End-to-end automation and Neural Architecture Search strategies. | RL, Evolutionary, Gradient-Based (DARTS) Search. |
| **Self-Supervised Learning** | Models learning from unlabeled data. | DINOv2 (7B params, 142M images), MAE, SimCLR. |
| **Transformers** | Evolution of the self-attention mechanism. | BERT, GPT (GPT-5 anticipated), ViT, Swin Transformer. |
| **CNN Evolution** | From AlexNet to EfficientNet. | ResNet (Residual connections), GoogLeNet (Inception), DenseNet. |
| **Attention Mechanisms** | Detailed breakdown of attention types. | Self-Attention, Multi-Head, Cross-Attention, SE Networks. |
| **Meta-Learning** | Learning to learn, few-shot capabilities. | MAML, Prototypical Networks, N-way K-shot. |
| **Compression** | Techniques for model deployment on mobile/edge devices. | Quantization (FP32‚ÜíINT8), Pruning, Knowledge Distillation. |

### Statistics

| Metric | Value |
| :--- | :--- |
| **Total URLs** | 200+ |
| **Architectures** | 100+ |
| **Techniques** | 500+ |
| **Academic Papers** | 150+ |
| **Content Size** | 400+ pages |

### Resources (Selected)

- **DINOv2 (Meta AI):** Self-Supervised Champion with 7 billion parameters.
- **ResNet (2015):** Revolutionized CNNs with residual connections.
- **DARTS:** Differentiable Architecture Search.
- **Attention(Q,K,V):** The core mathematical formula for attention.

---

## üáßüá∑ Portugu√™s

### Vis√£o Geral

Esta √© a cole√ß√£o mais abrangente de recursos, arquiteturas e t√©cnicas de Deep Learning, compilada a partir de uma busca massiva e recorde. Ela cobre a evolu√ß√£o de conceitos chave de 2012 a 2025, incluindo Transformers, CNNs, Aprendizado Auto-Supervisionado e AutoML.

### Se√ß√µes Principais

| Se√ß√£o | Descri√ß√£o | Destaques Principais |
| :--- | :--- | :--- |
| **AutoML & NAS** | Automa√ß√£o ponta a ponta e estrat√©gias de Busca de Arquitetura Neural. | RL, Evolucion√°rio, Busca Baseada em Gradiente (DARTS). |
| **Aprendizado Auto-Supervisionado** | Modelos que aprendem a partir de dados n√£o rotulados. | DINOv2 (7B params, 142M imagens), MAE, SimCLR. |
| **Transformers** | Evolu√ß√£o do mecanismo de auto-aten√ß√£o. | BERT, GPT (GPT-5 antecipado), ViT, Swin Transformer. |
| **Evolu√ß√£o das CNNs** | De AlexNet a EfficientNet. | ResNet (Conex√µes residuais), GoogLeNet (Inception), DenseNet. |
| **Mecanismos de Aten√ß√£o** | Detalhamento dos tipos de aten√ß√£o. | Auto-Aten√ß√£o, Multi-Head, Cross-Attention, SE Networks. |
| **Meta-Aprendizado** | Aprender a aprender, capacidades few-shot. | MAML, Redes Protot√≠picas, N-way K-shot. |
| **Compress√£o** | T√©cnicas para implanta√ß√£o de modelos em dispositivos m√≥veis/edge. | Quantiza√ß√£o (FP32‚ÜíINT8), Poda (Pruning), Destila√ß√£o de Conhecimento. |

### Estat√≠sticas

| M√©trica | Valor |
| :--- | :--- |
| **Total de URLs** | 200+ |
| **Arquiteturas** | 100+ |
| **T√©cnicas** | 500+ |
| **Papers Acad√™micos** | 150+ |
| **Tamanho do Conte√∫do** | 400+ p√°ginas |

### Recursos (Selecionados)

- **DINOv2 (Meta AI):** Campe√£o Auto-Supervisionado com 7 bilh√µes de par√¢metros.
- **ResNet (2015):** Revolucionou as CNNs com conex√µes residuais.
- **DARTS:** Busca Diferenci√°vel de Arquitetura.
- **Attention(Q,K,V):** A f√≥rmula matem√°tica central para aten√ß√£o.

---
**Refer√™ncias:**
(As refer√™ncias originais do arquivo anexado foram omitidas para manter o foco no conte√∫do principal, mas podem ser adicionadas se necess√°rio.)
